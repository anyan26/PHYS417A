{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to FCN (Fully Convolutional Network)"
      ],
      "metadata": {
        "id": "1n7eZUU9-wjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Shallow vs Deep Networks**\n",
        "\n",
        "shallow: 1 hidden layer ex: iris\n",
        "\n",
        "*   theoretically can approximate with enough hidden neurons\n",
        "*   fit for simple problem (easier to train)\n",
        "\n",
        "\n",
        "Deep: > 1 hidden layer\n",
        "*   upper hidden neurons reuse lower-level features\n",
        "*   Can approximate more complex and general functions\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8YWCjK87_8hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression vs. Classification**\n",
        "\n",
        "Regression output a variable\n",
        "\n",
        "Classification output a category"
      ],
      "metadata": {
        "id": "n8pjmiOVAYXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax**\n",
        "\n",
        "torch.nn.Softmax() -> probability"
      ],
      "metadata": {
        "id": "m9c-cUf4Ab-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Additional Data Prep Methods\n",
        "**TRAINING/VALIDATION/TEST SETS**\n",
        "* training set and testing set\n",
        "* training divide into training and validation\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state = 2)\n",
        "\n",
        "**ONE-HOT ENCODING**\n",
        "* convert human-readable categories into machine readable vectors\n"
      ],
      "metadata": {
        "id": "6Js3jwPEAdfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "labels = torch.tensor([0, 1, 2]) #only integers\n",
        "\n",
        "labels_one_hot = torch.nn.functional.one_hot(labels)\n",
        "print(labels_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60mO6pMXDtd3",
        "outputId": "e34b0243-172e-4579-fe65-63384e78ffe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 0, 0],\n",
            "        [0, 1, 0],\n",
            "        [0, 0, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stochastic Gradient Descent\n",
        "- SGD, Mini-batch GD, Batch GD"
      ],
      "metadata": {
        "id": "H8WnHvS_EGdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing SGD\n",
        "for epoch in range(epochs):\n",
        "  for sample in train:\n",
        "    training loop\n",
        "\n",
        "#implementing mini-batch\n",
        "for epoch in range(epochs):\n",
        "  for mini-batch in train:\n",
        "    training loop\n",
        "\n",
        "#implementing batchGD\n",
        "for epoch in range(epochs):\n",
        "  training loop"
      ],
      "metadata": {
        "id": "GnghTzicEsWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Additional Hyper-parameters and Regularizers\n",
        "\n"
      ],
      "metadata": {
        "id": "oPonnmYoPKfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Activation Functions**\n",
        "\n",
        "Sigmoid: torch.nn.functional.sigmoid()\n",
        "\n",
        "ReLU: torch.nn.functional.relu()\n",
        "\n",
        "tanh: torch.nn.functional.tanh()\n",
        "\n",
        "Leaky ReLU: torch.nn.functional.leaky_relu()\n",
        "\n",
        "\n",
        "out =  torch.nn.functional.sigmoid(self.layer1(x))\n",
        "\n",
        "output = torch.nn.functional.softmax(self.layer2(out1), dim=1) #number in each column"
      ],
      "metadata": {
        "id": "GfdzzdXtPYlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function**\n",
        "\n",
        "MSE: torch.nn.MSELoss()\n",
        "cross entropy: torch.nn.CrossEntropyLoss(), pyTorch automatically implement one-hot torching"
      ],
      "metadata": {
        "id": "dgj7X0vNP69l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Optimizers**\n",
        "Gradient descent: torch.optim.SGD()\n",
        "\n",
        "Adam: torch.optim.Adam(), frequently used\n",
        "\n",
        "RMSProp: torch.optim.RMSprop()\n",
        "\n",
        "AdaDelta: torch.optim.Adaelta()"
      ],
      "metadata": {
        "id": "79tFKebdQMjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Avoid Overfitting with Regularization**\n"
      ],
      "metadata": {
        "id": "iroKSx9XQfzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#L1 implementation\n",
        "l1_penalty = torch.nn.L1Loss(size_average=False)\n",
        "reg_loss = 0\n",
        "\n",
        "for param in model.parameters():\n",
        "  reg_loss += l1_penalty(param)\n",
        "\n",
        "lambda_ = 0.9\n",
        "loss += lambda_ * reg_loss"
      ],
      "metadata": {
        "id": "AQGeapQqRkBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#L2 is already implemented in pytorch\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay = 0.9)\n",
        "#lr = learning rate"
      ],
      "metadata": {
        "id": "EifVR9IRR02W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout Regularization\n",
        "\n",
        "randomly disabling neurons -> force neurons to learn together"
      ],
      "metadata": {
        "id": "8SVOZADNR_az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#implementation of dropout\n",
        "\n",
        "#in __init__\n",
        "self.dropout = torch.nn.Dropout(p=0.25)\n",
        "\n",
        "#in forward\n",
        "out = self.dropout(out1)"
      ],
      "metadata": {
        "id": "jDZZS4uFSjR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preventing Vanishing/Exploding Gradients\n",
        "\n"
      ],
      "metadata": {
        "id": "GfXfvlJ5S1o-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Normalization**\n",
        "normalize the columns\n",
        "\n",
        "in init:\n",
        "\n",
        "self.bn1 = torch.nn.BatchNorm1d(5) //5 is number of dimension\n",
        "\n",
        "in forward:\n",
        "out1 = self.bn1(out1)\n",
        "\n",
        "BATCH NORMALIZATION IS DONE BEFORE FEEDING INTO ACTIVATING FUNCTION"
      ],
      "metadata": {
        "id": "YMxUtMwrS7Lc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "syntax:\n",
        "in init: torch.nn.init.FUNCTION(self.layer.weight)"
      ],
      "metadata": {
        "id": "3o2FlWY9TXMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Fully Connected Networks"
      ],
      "metadata": {
        "id": "X0W6nI7VT3f_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define model\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.15, random_state = 2)\n",
        "\n",
        "X_validation = X_train[:int(len(X_test))]\n",
        "y_validation = y_train[:int(len(y_test))]\n",
        "\n",
        "X_train = X_train[int(len(X_test)):]\n",
        "y_train = y_train[(int(len(y_test))):]\n",
        "\n",
        "class irisClassificationFCN(torch.nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, hidden1_dim, hidden2_dim):\n",
        "    super(irisCalssifcationFCN, self).__init__()\n",
        "\n",
        "    self.layer1 = torch.nn.Linear(input_dim, hidden1_dim)\n",
        "    self.layer2 = torch.nn.Linear(hidden1_dim, hidden2_dim)\n",
        "    self.layer3 = torch.nn.Linear(hidden2_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out1 = torch.nn.functional.relu(self.layer1(x))\n",
        "    out2 = torch.nn.functional.relu(self.layer2(out1))\n",
        "    output = torch.nn.functional.softmax(self.layer3(out2), dim=1)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "luRz0MiXT9mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm #visualize progress of training\n",
        "\n",
        "#compute validation accuracy\n",
        "with torch.no_grad():\n",
        "  validation_outputs = model(validation_inputs)\n",
        "  correct = (torch.argmax(validation_outputs, dim=1) == validation_targets).type(torch.FloatTensor)\n",
        "\n",
        "  validation_accuracy_list[epoch] = correct.mean()"
      ],
      "metadata": {
        "id": "EA1U7LD1WE1b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}